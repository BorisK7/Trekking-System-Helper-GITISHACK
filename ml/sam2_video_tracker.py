#!/usr/bin/env python3
"""
ü§ñ SAM 2 Video Object Segmentation
===================================
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SAM 2.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    pip install git+https://github.com/facebookresearch/segment-anything-2.git
    
–ó–∞–ø—É—Å–∫:
    python sam2_video_tracker.py video.mp4 --auto
    python sam2_video_tracker.py video.mp4 --interactive
"""

import cv2
import numpy as np
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Tuple, Any
from collections import deque
import colorsys
import time
import tempfile
import shutil
import os


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üé® –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_vibrant_colors(n: int) -> List[Tuple[int, int, int]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —è—Ä–∫–∏–µ –Ω–∞—Å—ã—â–µ–Ω–Ω—ã–µ —Ü–≤–µ—Ç–∞"""
    colors = []
    for i in range(n):
        hue = (i * 0.618033988749895) % 1.0  # Golden ratio
        sat = 0.85 + 0.15 * np.sin(i * 0.7)
        val = 0.95
        rgb = colorsys.hsv_to_rgb(hue, sat, val)
        colors.append(tuple(int(c * 255) for c in rgb[::-1]))  # BGR
    return colors


class SAMVisualizer:
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è SAM –º–∞—Å–æ–∫"""
    
    def __init__(self, num_colors: int = 30):
        self.colors = generate_vibrant_colors(num_colors)
        
    def apply_mask(self, frame: np.ndarray, mask: np.ndarray, 
                   color: Tuple[int, int, int], alpha: float = 0.5) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Å–∫—É —Å —Ü–≤–µ—Ç–æ–º"""
        overlay = frame.copy()
        overlay[mask > 0] = color
        return cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)
    
    def draw_contour(self, frame: np.ndarray, mask: np.ndarray,
                     color: Tuple[int, int, int], thickness: int = 2) -> np.ndarray:
        """–†–∏—Å—É–µ—Ç –∫–æ–Ω—Ç—É—Ä –º–∞—Å–∫–∏"""
        contours, _ = cv2.findContours(mask.astype(np.uint8), 
                                       cv2.RETR_EXTERNAL, 
                                       cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(frame, contours, -1, (255, 255, 255), thickness + 1)
        cv2.drawContours(frame, contours, -1, color, thickness)
        return frame
    
    def draw_glow(self, frame: np.ndarray, mask: np.ndarray,
                  color: Tuple[int, int, int], blur_size: int = 25) -> np.ndarray:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç —Å–≤–µ—á–µ–Ω–∏—è"""
        glow_mask = cv2.GaussianBlur(mask.astype(np.float32) * 255, 
                                     (blur_size, blur_size), 0)
        glow_mask = np.clip(glow_mask / 255.0 * 0.5, 0, 1)
        
        glow_layer = np.zeros_like(frame, dtype=np.float32)
        for i in range(3):
            glow_layer[:, :, i] = glow_mask * color[i]
        
        result = frame.astype(np.float32) + glow_layer
        return np.clip(result, 0, 255).astype(np.uint8)
    
    def visualize_all(self, frame: np.ndarray, 
                      masks: Dict[int, np.ndarray],
                      trajectories: Optional[Dict[int, List]] = None) -> np.ndarray:
        """–ü–æ–ª–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤"""
        output = frame.copy()
        
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∏—Å—É–µ–º glow —ç—Ñ—Ñ–µ–∫—Ç—ã
        for obj_id, mask in masks.items():
            color = self.colors[obj_id % len(self.colors)]
            output = self.draw_glow(output, mask, color)
        
        # –ó–∞—Ç–µ–º –º–∞—Å–∫–∏
        for obj_id, mask in masks.items():
            color = self.colors[obj_id % len(self.colors)]
            output = self.apply_mask(output, mask, color, alpha=0.35)
        
        # –ö–æ–Ω—Ç—É—Ä—ã
        for obj_id, mask in masks.items():
            color = self.colors[obj_id % len(self.colors)]
            output = self.draw_contour(output, mask, color)
        
        # –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        if trajectories:
            for obj_id, points in trajectories.items():
                if len(points) < 2:
                    continue
                color = self.colors[obj_id % len(self.colors)]
                for i in range(1, len(points)):
                    alpha = i / len(points)
                    pt_color = tuple(int(c * alpha) for c in color)
                    thickness = max(1, int(3 * alpha))
                    pt1 = tuple(map(int, points[i-1]))
                    pt2 = tuple(map(int, points[i]))
                    cv2.line(output, pt1, pt2, pt_color, thickness)
        
        # –¶–µ–Ω—Ç—Ä–æ–∏–¥—ã –∏ –º–µ—Ç–∫–∏
        for obj_id, mask in masks.items():
            if not mask.any():
                continue
            
            color = self.colors[obj_id % len(self.colors)]
            
            # –ù–∞—Ö–æ–¥–∏–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥
            contours, _ = cv2.findContours(mask.astype(np.uint8),
                                          cv2.RETR_EXTERNAL,
                                          cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                largest = max(contours, key=cv2.contourArea)
                M = cv2.moments(largest)
                if M["m00"] > 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    
                    # –¶–µ–Ω—Ç—Ä–æ–∏–¥
                    cv2.circle(output, (cx, cy), 8, (255, 255, 255), 2)
                    cv2.circle(output, (cx, cy), 6, color, -1)
                    
                    # –ú–µ—Ç–∫–∞
                    label = f"Object {obj_id}"
                    cv2.putText(output, label, (cx + 15, cy - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3)
                    cv2.putText(output, label, (cx + 15, cy - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return output


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ü§ñ SAM 2 WRAPPER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class SAM2VideoTracker:
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ SAM 2 –¥–ª—è video object segmentation.
    """
    
    def __init__(self, model_size: str = "large", device: str = "auto"):
        """
        Args:
            model_size: 'tiny', 'small', 'base_plus', 'large'
            device: 'cuda', 'mps', 'cpu', 'auto'
        """
        self.model_size = model_size
        self.device = self._get_device(device)
        self.predictor = None
        self.inference_state = None
        self.video_segments = {}
        self.object_ids = []
        self.visualizer = SAMVisualizer()
        self.trajectories: Dict[int, deque] = {}
        
        self._temp_dir = None
        
    def _get_device(self, device: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if device == "auto":
            import torch
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            return "cpu"
        return device
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç SAM 2 –º–æ–¥–µ–ª—å"""
        try:
            import torch
            from sam2.build_sam import build_sam2_video_predictor
            
            model_configs = {
                'tiny': 'sam2_hiera_t.yaml',
                'small': 'sam2_hiera_s.yaml', 
                'base_plus': 'sam2_hiera_b+.yaml',
                'large': 'sam2_hiera_l.yaml'
            }
            
            checkpoints = {
                'tiny': 'facebook/sam2-hiera-tiny',
                'small': 'facebook/sam2-hiera-small',
                'base_plus': 'facebook/sam2-hiera-base-plus',
                'large': 'facebook/sam2-hiera-large'
            }
            
            config = model_configs.get(self.model_size, 'sam2_hiera_l.yaml')
            checkpoint = checkpoints.get(self.model_size, 'facebook/sam2-hiera-large')
            
            print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ SAM 2 ({self.model_size}) –Ω–∞ {self.device}...")
            
            self.predictor = build_sam2_video_predictor(
                config, 
                checkpoint,
                device=self.device
            )
            
            print(f"‚úì SAM 2 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ SAM 2: {e}")
            print("\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SAM 2:")
            print("  pip install git+https://github.com/facebookresearch/segment-anything-2.git")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def _extract_frames(self, video_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞–¥—Ä—ã –∏–∑ –≤–∏–¥–µ–æ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é"""
        self._temp_dir = tempfile.mkdtemp(prefix="sam2_frames_")
        
        cap = cv2.VideoCapture(video_path)
        frame_idx = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_path = os.path.join(self._temp_dir, f"{frame_idx:06d}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_idx += 1
        
        cap.release()
        print(f"üìÅ –ò–∑–≤–ª–µ—á–µ–Ω–æ {frame_idx} –∫–∞–¥—Ä–æ–≤")
        return self._temp_dir
    
    def init_video(self, video_path: str):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞"""
        if self.predictor is None:
            if not self._load_model():
                raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å SAM 2")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã
        frames_dir = self._extract_frames(video_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º state
        self.inference_state = self.predictor.init_state(video_path=frames_dir)
        self.video_segments = {}
        self.object_ids = []
        
        print(f"‚úì –í–∏–¥–µ–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
    
    def add_object_by_point(self, frame_idx: int, point: Tuple[int, int],
                           obj_id: Optional[int] = None) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—ä–µ–∫—Ç –ø–æ —Ç–æ—á–∫–µ.
        
        Args:
            frame_idx: –ò–Ω–¥–µ–∫—Å –∫–∞–¥—Ä–∞
            point: –¢–æ—á–∫–∞ (x, y) –Ω–∞ –æ–±—ä–µ–∫—Ç–µ
            obj_id: ID –æ–±—ä–µ–∫—Ç–∞ (–µ—Å–ª–∏ None - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π)
        
        Returns:
            ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
        """
        if obj_id is None:
            obj_id = len(self.object_ids)
        
        points = np.array([[point]], dtype=np.float32)
        labels = np.array([[1]], dtype=np.int32)
        
        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
            inference_state=self.inference_state,
            frame_idx=frame_idx,
            obj_id=obj_id,
            points=points,
            labels=labels,
        )
        
        if obj_id not in self.object_ids:
            self.object_ids.append(obj_id)
            self.trajectories[obj_id] = deque(maxlen=100)
        
        print(f"‚úì –î–æ–±–∞–≤–ª–µ–Ω –æ–±—ä–µ–∫—Ç {obj_id} –ø–æ —Ç–æ—á–∫–µ {point}")
        return obj_id
    
    def add_object_by_box(self, frame_idx: int, box: Tuple[int, int, int, int],
                         obj_id: Optional[int] = None) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—ä–µ–∫—Ç –ø–æ bounding box.
        
        Args:
            frame_idx: –ò–Ω–¥–µ–∫—Å –∫–∞–¥—Ä–∞
            box: Bounding box (x1, y1, x2, y2)
            obj_id: ID –æ–±—ä–µ–∫—Ç–∞
        
        Returns:
            ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
        """
        if obj_id is None:
            obj_id = len(self.object_ids)
        
        box_np = np.array([box], dtype=np.float32)
        
        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
            inference_state=self.inference_state,
            frame_idx=frame_idx,
            obj_id=obj_id,
            box=box_np,
        )
        
        if obj_id not in self.object_ids:
            self.object_ids.append(obj_id)
            self.trajectories[obj_id] = deque(maxlen=100)
        
        print(f"‚úì –î–æ–±–∞–≤–ª–µ–Ω –æ–±—ä–µ–∫—Ç {obj_id} –ø–æ box {box}")
        return obj_id
    
    def propagate(self) -> Dict[int, Dict[int, np.ndarray]]:
        """
        –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –≤—Å–µ –∫–∞–¥—Ä—ã.
        
        Returns:
            Dict[frame_idx, Dict[obj_id, mask]]
        """
        print("üîÑ –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏...")
        
        self.video_segments = {}
        
        for out_frame_idx, out_obj_ids, out_mask_logits in \
                self.predictor.propagate_in_video(self.inference_state):
            
            self.video_segments[out_frame_idx] = {}
            
            for i, obj_id in enumerate(out_obj_ids):
                mask = (out_mask_logits[i] > 0.0).cpu().numpy().squeeze()
                self.video_segments[out_frame_idx][obj_id] = mask
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
                if mask.any():
                    ys, xs = np.where(mask)
                    centroid = (int(xs.mean()), int(ys.mean()))
                    self.trajectories[obj_id].append(centroid)
        
        print(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.video_segments)} –∫–∞–¥—Ä–æ–≤")
        return self.video_segments
    
    def process_and_save(self, input_path: str, output_path: str,
                         show_preview: bool = True):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        
        Args:
            input_path: –í—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ
            output_path: –í—ã—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ
            show_preview: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–≤—å—é
        """
        cap = cv2.VideoCapture(input_path)
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        print(f"\n{'='*50}")
        print("üé¨ –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
        print(f"{'='*50}\n")
        
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # –ü–æ–ª—É—á–∞–µ–º –º–∞—Å–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–∞–¥—Ä–∞
            masks = self.video_segments.get(frame_idx, {})
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞
            current_trajectories = {}
            for obj_id, traj in self.trajectories.items():
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ—á–∫–∏ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞
                current_trajectories[obj_id] = list(traj)[:frame_idx + 1]
            
            # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
            output_frame = self.visualizer.visualize_all(
                frame, masks, current_trajectories
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            info = f"Frame: {frame_idx + 1}/{total_frames} | Objects: {len(masks)}"
            cv2.putText(output_frame, info, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)
            cv2.putText(output_frame, info, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
            
            out.write(output_frame)
            
            if show_preview:
                preview = cv2.resize(output_frame, (0, 0), fx=0.5, fy=0.5)
                cv2.imshow('SAM 2 Tracking', preview)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            if frame_idx % 30 == 0:
                progress = (frame_idx + 1) / total_frames * 100
                print(f"\r  Progress: {progress:.1f}%", end="")
            
            frame_idx += 1
        
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        
        print(f"\n\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
    
    def cleanup(self):
        """–û—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        if self._temp_dir and os.path.exists(self._temp_dir):
            shutil.rmtree(self._temp_dir)
            self._temp_dir = None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üéØ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –î–ï–¢–ï–ö–¶–ò–Ø –û–ë–™–ï–ö–¢–û–í
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AutoObjectDetector:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂—É—â–∏—Ö—Å—è –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è SAM 2.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç optical flow –∏ background subtraction –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤.
    """
    
    def __init__(self, min_area: int = 1000, max_objects: int = 10):
        self.min_area = min_area
        self.max_objects = max_objects
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            history=200, varThreshold=50, detectShadows=True
        )
    
    def detect_moving_objects(self, frames: List[np.ndarray], 
                              num_frames: int = 30) -> List[Dict]:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –¥–≤–∏–∂—É—â–∏–µ—Å—è –æ–±—ä–µ–∫—Ç—ã –≤ –ø–µ—Ä–≤—ã—Ö –∫–∞–¥—Ä–∞—Ö.
        
        Args:
            frames: –°–ø–∏—Å–æ–∫ –∫–∞–¥—Ä–æ–≤
            num_frames: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
        """
        detections_per_frame = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –∫–∞–¥—Ä—ã
        for frame in frames[:num_frames]:
            fg_mask = self.bg_subtractor.apply(frame)
            fg_mask[fg_mask == 127] = 0  # –£–¥–∞–ª—è–µ–º —Ç–µ–Ω–∏
            
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
            
            contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL,
                                          cv2.CHAIN_APPROX_SIMPLE)
            
            frame_detections = []
            for contour in contours:
                area = cv2.contourArea(contour)
                if area < self.min_area:
                    continue
                
                x, y, w, h = cv2.boundingRect(contour)
                M = cv2.moments(contour)
                if M["m00"] > 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                else:
                    cx, cy = x + w // 2, y + h // 2
                
                frame_detections.append({
                    'bbox': (x, y, x + w, y + h),
                    'centroid': (cx, cy),
                    'area': area
                })
            
            detections_per_frame.append(frame_detections)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–¥—Ä–∞—Ö)
        stable_objects = self._find_stable_objects(detections_per_frame)
        
        return stable_objects[:self.max_objects]
    
    def _find_stable_objects(self, detections_per_frame: List[List[Dict]],
                             min_frames: int = 5, max_distance: int = 100) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—ä–µ–∫—Ç—ã, —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–¥—Ä–∞—Ö"""
        if not detections_per_frame:
            return []
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –¥–µ—Ç–µ–∫—Ü–∏–π –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        mid_idx = len(detections_per_frame) // 2
        if not detections_per_frame[mid_idx]:
            return []
        
        stable = []
        
        for det in detections_per_frame[mid_idx]:
            count = 1
            total_cx, total_cy = det['centroid']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤ –¥—Ä—É–≥–∏—Ö –∫–∞–¥—Ä–∞—Ö
            for i, frame_dets in enumerate(detections_per_frame):
                if i == mid_idx:
                    continue
                
                for other_det in frame_dets:
                    dist = np.sqrt(
                        (det['centroid'][0] - other_det['centroid'][0])**2 +
                        (det['centroid'][1] - other_det['centroid'][1])**2
                    )
                    if dist < max_distance:
                        count += 1
                        total_cx += other_det['centroid'][0]
                        total_cy += other_det['centroid'][1]
                        break
            
            if count >= min_frames:
                avg_centroid = (total_cx // count, total_cy // count)
                stable.append({
                    'centroid': avg_centroid,
                    'bbox': det['bbox'],
                    'stability': count
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        stable.sort(key=lambda x: x['stability'], reverse=True)
        
        return stable


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üé¨ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class InteractiveSAM2:
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≤—ã–±–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è SAM 2"""
    
    def __init__(self, tracker: SAM2VideoTracker):
        self.tracker = tracker
        self.current_frame = None
        self.current_frame_idx = 0
        self.selected_points = []
        self.preview_mask = None
        
    def mouse_callback(self, event, x, y, flags, param):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –º—ã—à–∏"""
        if event == cv2.EVENT_LBUTTONDOWN:
            self.selected_points.append((x, y))
            print(f"  Point added: ({x}, {y})")
    
    def select_objects(self, video_path: str) -> List[Dict]:
        """
        –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –æ–±—ä–µ–∫—Ç–æ–≤.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Ç–æ—á–∫–∞–º–∏
        """
        cap = cv2.VideoCapture(video_path)
        ret, self.current_frame = cap.read()
        cap.release()
        
        if not ret:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤–∏–¥–µ–æ")
            return []
        
        window_name = "Select Objects (LMB=add, N=next object, ENTER=done, Q=quit)"
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        cv2.setMouseCallback(window_name, self.mouse_callback)
        
        print("\n" + "="*50)
        print("üéØ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –í–´–ë–û–† –û–ë–™–ï–ö–¢–û–í")
        print("="*50)
        print("  LMB   - –¥–æ–±–∞–≤–∏—Ç—å —Ç–æ—á–∫—É –æ–±—ä–µ–∫—Ç–∞")
        print("  N     - —Å–ª–µ–¥—É—é—â–∏–π –æ–±—ä–µ–∫—Ç")
        print("  D     - —É–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É")
        print("  ENTER - –∑–∞–≤–µ—Ä—à–∏—Ç—å –≤—ã–±–æ—Ä")
        print("  Q     - –æ—Ç–º–µ–Ω–∞")
        print("="*50 + "\n")
        
        objects = []
        current_object_points = []
        
        while True:
            display = self.current_frame.copy()
            
            # –†–∏—Å—É–µ–º —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
            for i, obj in enumerate(objects):
                color = self.tracker.visualizer.colors[i % len(self.tracker.visualizer.colors)]
                for pt in obj['points']:
                    cv2.circle(display, pt, 8, (255, 255, 255), 2)
                    cv2.circle(display, pt, 6, color, -1)
            
            # –†–∏—Å—É–µ–º —Ç–µ–∫—É—â–∏–µ —Ç–æ—á–∫–∏
            color = self.tracker.visualizer.colors[len(objects) % len(self.tracker.visualizer.colors)]
            for pt in self.selected_points:
                cv2.circle(display, pt, 8, (255, 255, 255), 2)
                cv2.circle(display, pt, 6, color, -1)
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            info = f"Objects: {len(objects)} | Current points: {len(self.selected_points)}"
            cv2.putText(display, info, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3)
            cv2.putText(display, info, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
            
            cv2.imshow(window_name, display)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("  –û—Ç–º–µ–Ω–∞")
                cv2.destroyAllWindows()
                return []
            
            elif key == 13:  # ENTER
                if self.selected_points:
                    objects.append({'points': self.selected_points.copy()})
                    self.selected_points = []
                break
            
            elif key == ord('n'):
                if self.selected_points:
                    objects.append({'points': self.selected_points.copy()})
                    self.selected_points = []
                    print(f"  Object {len(objects)} saved")
            
            elif key == ord('d'):
                if self.selected_points:
                    self.selected_points.pop()
                    print("  Last point removed")
        
        cv2.destroyAllWindows()
        print(f"\n‚úì –í—ã–±—Ä–∞–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(objects)}")
        
        return objects


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üöÄ MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ü§ñ SAM 2 Video Object Segmentation',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('input', type=str, help='Input video path')
    parser.add_argument('-o', '--output', type=str, default=None,
                       help='Output video path')
    parser.add_argument('--auto', action='store_true',
                       help='Auto-detect moving objects')
    parser.add_argument('--interactive', action='store_true',
                       help='Interactive object selection')
    parser.add_argument('--model', type=str, default='large',
                       choices=['tiny', 'small', 'base_plus', 'large'],
                       help='SAM 2 model size')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['cuda', 'mps', 'cpu', 'auto'],
                       help='Device to use')
    parser.add_argument('--no-preview', action='store_true',
                       help='Disable preview')
    
    args = parser.parse_args()
    
    # –í—ã—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å
    if args.output is None:
        input_path = Path(args.input)
        args.output = str(input_path.parent / f"{input_path.stem}_sam2.mp4")
    
    print(f"\n{'‚ïê'*60}")
    print(f"  ü§ñ SAM 2 VIDEO OBJECT SEGMENTATION")
    print(f"{'‚ïê'*60}")
    print(f"  üìÅ Input:  {args.input}")
    print(f"  üìÅ Output: {args.output}")
    print(f"  üß† Model:  {args.model}")
    print(f"  üíª Device: {args.device}")
    print(f"{'‚ïê'*60}\n")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–∫–µ—Ä
    tracker = SAM2VideoTracker(model_size=args.model, device=args.device)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∏–¥–µ–æ
        tracker.init_video(args.input)
        
        if args.interactive:
            # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
            interactive = InteractiveSAM2(tracker)
            objects = interactive.select_objects(args.input)
            
            if not objects:
                print("‚ùå –û–±—ä–µ–∫—Ç—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã")
                return
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç—ã
            for i, obj in enumerate(objects):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é —Ç–æ—á–∫—É –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
                point = obj['points'][0]
                tracker.add_object_by_point(0, point, obj_id=i)
        
        elif args.auto:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º
            print("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤...")
            
            # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ –∫–∞–¥—Ä—ã
            cap = cv2.VideoCapture(args.input)
            frames = []
            for _ in range(50):
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
            cap.release()
            
            # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã
            detector = AutoObjectDetector(min_area=1500, max_objects=8)
            detected = detector.detect_moving_objects(frames)
            
            if not detected:
                print("‚ö† –î–≤–∏–∂—É—â–∏–µ—Å—è –æ–±—ä–µ–∫—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
                print("  –ü–æ–ø—Ä–æ–±—É–π—Ç–µ --interactive —Ä–µ–∂–∏–º")
                return
            
            print(f"‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(detected)}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç—ã –≤ SAM 2
            for i, obj in enumerate(detected):
                tracker.add_object_by_point(0, obj['centroid'], obj_id=i)
        
        else:
            print("‚ö† –£–∫–∞–∂–∏—Ç–µ —Ä–µ–∂–∏–º: --auto –∏–ª–∏ --interactive")
            return
        
        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        tracker.propagate()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        tracker.process_and_save(args.input, args.output, 
                                show_preview=not args.no_preview)
        
    finally:
        tracker.cleanup()


if __name__ == '__main__':
    main()

