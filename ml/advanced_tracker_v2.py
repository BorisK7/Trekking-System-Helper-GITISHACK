#!/usr/bin/env python3
"""
ğŸ­ Advanced Theater Tracker v2 - Ğ¡ MERGE ĞĞ‘ĞªĞ•ĞšĞ¢ĞĞ’
===================================================
Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾:
- Merge Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²
- Merge Ğ¿Ğ¾ ÑÑ…Ğ¾Ğ¶ĞµĞ¼Ñƒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ
- Merge Ğ¿Ğ¾ IoU (Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ bbox)
- Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹

Ğ—Ğ°Ğ¿ÑƒÑĞº:
    python advanced_tracker_v2.py test.mp4
    python advanced_tracker_v2.py test.mp4 --merge-distance 100
    python advanced_tracker_v2.py test.mp4 --merge-velocity 0.8
"""

import cv2
import numpy as np
from pathlib import Path
from collections import deque
from dataclasses import dataclass, field
from typing import List, Tuple, Dict, Optional, Set
from scipy.cluster.hierarchy import fclusterdata
from scipy.spatial.distance import pdist, squareform
import colorsys
import time


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class MergeConfig:
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²"""
    
    # === Merge Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ===
    merge_distance: float = 80.0       # ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ merge (px)
    merge_enabled: bool = True
    
    # === Merge Ğ¿Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ===
    velocity_merge_enabled: bool = True
    velocity_similarity_threshold: float = 0.7  # ĞšĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (0-1)
    velocity_distance_factor: float = 1.5       # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼
    
    # === Merge Ğ¿Ğ¾ IoU ===
    iou_merge_enabled: bool = True
    iou_threshold: float = 0.1         # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ IoU Ğ´Ğ»Ñ merge
    
    # === Merge Ğ¿Ğ¾ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»Ğ¸ (Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº = Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ° + Ñ‚Ğ¾Ñ€Ñ + Ğ½Ğ¾Ğ³Ğ¸) ===
    vertical_merge_enabled: bool = True
    vertical_gap_max: float = 50.0     # ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²
    horizontal_overlap_min: float = 0.5  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ
    
    # === ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ===
    use_clustering: bool = True
    cluster_distance_threshold: float = 100.0
    
    # === ĞœĞ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸ĞµĞ¹ ===
    use_morphological_merge: bool = True
    morph_close_size: int = 15         # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ´Ñ€Ğ° Ğ´Ğ»Ñ close (ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸)


@dataclass 
class TrackerConfig:
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ĞµĞºĞµÑ€Ğ°"""
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹
    brightness_min: int = 15
    brightness_max: int = 245
    min_area: int = 1500
    max_area: int = 200000
    min_aspect_ratio: float = 0.15
    max_aspect_ratio: float = 6.0
    
    # Temporal
    confirm_frames: int = 3
    lost_frames_max: int = 20
    max_velocity: float = 100.0
    
    # Kalman
    use_kalman: bool = True
    
    # Merge
    merge: MergeConfig = field(default_factory=MergeConfig)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”— ĞœĞĞ”Ğ£Ğ›Ğ¬ Ğ¡Ğ›Ğ˜Ğ¯ĞĞ˜Ğ¯ Ğ”Ğ•Ğ¢Ğ•ĞšĞ¦Ğ˜Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DetectionMerger:
    """
    ĞœĞ¾Ğ´ÑƒĞ»ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹.
    Ğ ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑÑ‚ĞµĞ¹.
    """
    
    def __init__(self, config: MergeConfig):
        self.config = config
    
    def merge_detections(self, detections: List[Dict], 
                         prev_velocities: Optional[Dict[int, Tuple[float, float]]] = None) -> List[Dict]:
        """
        Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹.
        
        Args:
            detections: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ [{'centroid': (x,y), 'bbox': (x,y,w,h), 'mask': ...}]
            prev_velocities: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ {idx: (vx, vy)}
        
        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹
        """
        if not detections or len(detections) < 2:
            return detections
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ³Ñ€Ğ°Ñ„ ÑĞ²ÑĞ·ĞµĞ¹
        n = len(detections)
        merge_graph = np.zeros((n, n), dtype=bool)
        
        for i in range(n):
            for j in range(i + 1, n):
                if self._should_merge(detections[i], detections[j], prev_velocities, i, j):
                    merge_graph[i, j] = True
                    merge_graph[j, i] = True
        
        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹
        clusters = self._find_connected_components(merge_graph)
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ
        merged = []
        for cluster in clusters:
            if len(cluster) == 1:
                merged.append(detections[cluster[0]])
            else:
                merged_det = self._merge_cluster([detections[i] for i in cluster])
                merged.append(merged_det)
        
        return merged
    
    def _should_merge(self, det1: Dict, det2: Dict,
                      velocities: Optional[Dict], idx1: int, idx2: int) -> bool:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ´Ğ²Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸"""
        
        c1, c2 = det1['centroid'], det2['centroid']
        b1, b2 = det1['bbox'], det2['bbox']
        
        # Ğ Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´Ğ°Ğ¼Ğ¸
        distance = np.sqrt((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)
        
        # === 1. Merge Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ===
        if self.config.merge_enabled:
            merge_dist = self.config.merge_distance
            
            # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ
            if velocities and self.config.velocity_merge_enabled:
                v1 = velocities.get(idx1)
                v2 = velocities.get(idx2)
                if v1 and v2:
                    similarity = self._velocity_similarity(v1, v2)
                    if similarity > self.config.velocity_similarity_threshold:
                        merge_dist *= self.config.velocity_distance_factor
            
            if distance < merge_dist:
                return True
        
        # === 2. Merge Ğ¿Ğ¾ IoU ===
        if self.config.iou_merge_enabled:
            iou = self._compute_iou(b1, b2)
            if iou > self.config.iou_threshold:
                return True
        
        # === 3. Merge Ğ¿Ğ¾ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»Ğ¸ (Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞ»Ğ°) ===
        if self.config.vertical_merge_enabled:
            if self._is_vertical_stack(b1, b2):
                return True
        
        # === 4. Merge Ğ¿Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ===
        if velocities and self.config.velocity_merge_enabled:
            v1 = velocities.get(idx1)
            v2 = velocities.get(idx2)
            if v1 and v2:
                similarity = self._velocity_similarity(v1, v2)
                # Ğ•ÑĞ»Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ°Ğ»ĞµĞºĞ¾
                if similarity > 0.9 and distance < self.config.merge_distance * 2:
                    return True
        
        return False
    
    def _velocity_similarity(self, v1: Tuple[float, float], 
                            v2: Tuple[float, float]) -> float:
        """
        Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ (ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾).
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ 0 Ğ´Ğ¾ 1.
        """
        mag1 = np.sqrt(v1[0]**2 + v1[1]**2)
        mag2 = np.sqrt(v2[0]**2 + v2[1]**2)
        
        # Ğ•ÑĞ»Ğ¸ Ğ¾Ğ±Ğ° Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ´Ğ²Ğ¸Ğ¶Ğ½Ñ‹
        if mag1 < 2 and mag2 < 2:
            return 1.0
        
        # Ğ•ÑĞ»Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ²Ğ¸Ğ¶ĞµÑ‚ÑÑ, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ½ĞµÑ‚
        if mag1 < 2 or mag2 < 2:
            return 0.5
        
        # ĞšĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾
        dot = v1[0] * v2[0] + v1[1] * v2[1]
        cos_sim = dot / (mag1 * mag2)
        
        # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ [0, 1]
        similarity = (cos_sim + 1) / 2
        
        # Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚ÑƒĞ´Ğµ
        mag_ratio = min(mag1, mag2) / max(mag1, mag2)
        
        # ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼
        return 0.7 * similarity + 0.3 * mag_ratio
    
    def _compute_iou(self, bbox1: Tuple, bbox2: Tuple) -> float:
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Intersection over Union Ğ´Ğ»Ñ Ğ´Ğ²ÑƒÑ… bbox"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ
        xi1 = max(x1, x2)
        yi1 = max(y1, y2)
        xi2 = min(x1 + w1, x2 + w2)
        yi2 = min(y1 + h1, y2 + h2)
        
        if xi2 <= xi1 or yi2 <= yi1:
            return 0.0
        
        intersection = (xi2 - xi1) * (yi2 - yi1)
        union = w1 * h1 + w2 * h2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def _is_vertical_stack(self, bbox1: Tuple, bbox2: Tuple) -> bool:
        """
        ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ»Ğ¸ bbox Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°.
        (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ° Ğ½Ğ°Ğ´ Ñ‚Ğ¾Ñ€ÑĞ¾Ğ¼)
        """
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ bbox
        if y1 < y2:
            upper, lower = bbox1, bbox2
        else:
            upper, lower = bbox2, bbox1
        
        ux, uy, uw, uh = upper
        lx, ly, lw, lh = lower
        
        # Ğ’ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸
        vertical_gap = ly - (uy + uh)
        
        if vertical_gap < 0 or vertical_gap > self.config.vertical_gap_max:
            return False
        
        # Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ
        overlap_left = max(ux, lx)
        overlap_right = min(ux + uw, lx + lw)
        
        if overlap_right <= overlap_left:
            return False
        
        overlap_width = overlap_right - overlap_left
        min_width = min(uw, lw)
        
        overlap_ratio = overlap_width / min_width
        
        return overlap_ratio >= self.config.horizontal_overlap_min
    
    def _find_connected_components(self, graph: np.ndarray) -> List[List[int]]:
        """ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ"""
        n = len(graph)
        visited = [False] * n
        components = []
        
        def dfs(node: int, component: List[int]):
            visited[node] = True
            component.append(node)
            for neighbor in range(n):
                if graph[node, neighbor] and not visited[neighbor]:
                    dfs(neighbor, component)
        
        for i in range(n):
            if not visited[i]:
                component = []
                dfs(i, component)
                components.append(component)
        
        return components
    
    def _merge_cluster(self, detections: List[Dict]) -> Dict:
        """ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ñƒ"""
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ bbox'Ñ‹
        x_min = min(d['bbox'][0] for d in detections)
        y_min = min(d['bbox'][1] for d in detections)
        x_max = max(d['bbox'][0] + d['bbox'][2] for d in detections)
        y_max = max(d['bbox'][1] + d['bbox'][3] for d in detections)
        
        merged_bbox = (x_min, y_min, x_max - x_min, y_max - y_min)
        
        # Ğ¦ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´ - Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸
        total_area = sum(d.get('area', d['bbox'][2] * d['bbox'][3]) for d in detections)
        cx = sum(d['centroid'][0] * d.get('area', d['bbox'][2] * d['bbox'][3]) 
                for d in detections) / total_area
        cy = sum(d['centroid'][1] * d.get('area', d['bbox'][2] * d['bbox'][3]) 
                for d in detections) / total_area
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸
        merged_mask = None
        for d in detections:
            if d.get('mask') is not None:
                if merged_mask is None:
                    merged_mask = d['mask'].copy()
                else:
                    merged_mask = cv2.bitwise_or(merged_mask, d['mask'])
        
        return {
            'centroid': (cx, cy),
            'bbox': merged_bbox,
            'mask': merged_mask,
            'area': total_area,
            'merged_from': len(detections)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”— ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ”Ğ•Ğ¢Ğ•ĞšĞ¦Ğ˜Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DetectionClusterer:
    """
    Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹.
    ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº merge.
    """
    
    def __init__(self, distance_threshold: float = 100.0):
        self.distance_threshold = distance_threshold
    
    def cluster(self, detections: List[Dict], 
                velocities: Optional[Dict[int, Tuple[float, float]]] = None) -> List[Dict]:
        """
        ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.
        """
        if len(detections) < 2:
            return detections
        
        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸: [x, y, vx, vy, w, h]
        features = []
        for i, det in enumerate(detections):
            cx, cy = det['centroid']
            w, h = det['bbox'][2], det['bbox'][3]
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            vx, vy = 0, 0
            if velocities and i in velocities:
                vx, vy = velocities[i]
            
            # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸
            features.append([
                cx / 100,  # position (scaled down)
                cy / 100,
                vx / 10,   # velocity
                vy / 10,
                w / 50,    # size
                h / 50
            ])
        
        features = np.array(features)
        
        # Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        try:
            clusters = fclusterdata(
                features, 
                t=self.distance_threshold / 100,  # scaled threshold
                criterion='distance',
                method='average'
            )
        except:
            # Ğ•ÑĞ»Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ
            return detections
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ğ¼
        cluster_groups = {}
        for i, cluster_id in enumerate(clusters):
            if cluster_id not in cluster_groups:
                cluster_groups[cluster_id] = []
            cluster_groups[cluster_id].append(i)
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼
        merger = DetectionMerger(MergeConfig())
        merged = []
        
        for indices in cluster_groups.values():
            if len(indices) == 1:
                merged.append(detections[indices[0]])
            else:
                cluster_dets = [detections[i] for i in indices]
                merged.append(merger._merge_cluster(cluster_dets))
        
        return merged


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ˆ KALMAN FILTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class KalmanTracker:
    """Kalman filter Ğ´Ğ»Ñ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸"""
    
    def __init__(self, initial_pos: Tuple[float, float]):
        self.kf = cv2.KalmanFilter(4, 2)
        
        self.kf.transitionMatrix = np.array([
            [1, 0, 1, 0],
            [0, 1, 0, 1],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ], dtype=np.float32)
        
        self.kf.measurementMatrix = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 0]
        ], dtype=np.float32)
        
        self.kf.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03
        self.kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * 0.1
        
        self.kf.statePost = np.array([
            [initial_pos[0]], [initial_pos[1]], [0], [0]
        ], dtype=np.float32)
        
        self.kf.errorCovPost = np.eye(4, dtype=np.float32)
    
    def predict(self) -> Tuple[float, float]:
        pred = self.kf.predict()
        return (float(pred[0]), float(pred[1]))
    
    def update(self, pos: Tuple[float, float]) -> Tuple[float, float]:
        measured = np.array([[pos[0]], [pos[1]]], dtype=np.float32)
        corrected = self.kf.correct(measured)
        return (float(corrected[0]), float(corrected[1]))
    
    def get_velocity(self) -> Tuple[float, float]:
        return (float(self.kf.statePost[2]), float(self.kf.statePost[3]))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ĞĞ¢Ğ¡Ğ›Ğ•Ğ–Ğ˜Ğ’ĞĞ•ĞœĞ«Ğ™ ĞĞ‘ĞªĞ•ĞšĞ¢
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class TrackedObject:
    """ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚"""
    
    id: int
    color: Tuple[int, int, int]
    
    centroid: Optional[Tuple[float, float]] = None
    bbox: Optional[Tuple[int, int, int, int]] = None
    mask: Optional[np.ndarray] = None
    
    trajectory: deque = field(default_factory=lambda: deque(maxlen=100))
    velocity: Tuple[float, float] = (0, 0)
    
    frames_seen: int = 0
    frames_lost: int = 0
    confirmed: bool = False
    
    kalman: Optional[KalmanTracker] = None
    smoothed_pos: Optional[Tuple[float, float]] = None
    
    merged_count: int = 1  # Ğ˜Ğ· ÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ ÑĞ¾Ğ±Ñ€Ğ°Ğ½
    
    def init_kalman(self):
        if self.centroid:
            self.kalman = KalmanTracker(self.centroid)
    
    def update(self, centroid: Tuple[float, float], 
               bbox: Tuple[int, int, int, int],
               mask: Optional[np.ndarray] = None,
               merged_count: int = 1):
        
        # Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ
        if self.centroid:
            self.velocity = (
                centroid[0] - self.centroid[0],
                centroid[1] - self.centroid[1]
            )
        
        # Kalman
        if self.kalman:
            self.kalman.predict()
            self.smoothed_pos = self.kalman.update(centroid)
        else:
            self.smoothed_pos = centroid
        
        self.centroid = centroid
        self.bbox = bbox
        self.mask = mask
        self.merged_count = merged_count
        
        self.trajectory.append(self.smoothed_pos or centroid)
        self.frames_seen += 1
        self.frames_lost = 0
    
    def predict_position(self) -> Optional[Tuple[float, float]]:
        if self.kalman:
            return self.kalman.predict()
        elif self.centroid:
            return (
                self.centroid[0] + self.velocity[0],
                self.centroid[1] + self.velocity[1]
            )
        return None
    
    def get_speed(self) -> float:
        return np.sqrt(self.velocity[0]**2 + self.velocity[1]**2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¬ Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ Ğ¢Ğ Ğ•ĞšĞ•Ğ  V2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AdvancedTrackerV2:
    """
    ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞºĞµÑ€ v2 Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹.
    """
    
    def __init__(self, config: Optional[TrackerConfig] = None):
        self.config = config or TrackerConfig()
        
        # ĞœĞ¾Ğ´ÑƒĞ»Ğ¸
        self.merger = DetectionMerger(self.config.merge)
        self.clusterer = DetectionClusterer(
            self.config.merge.cluster_distance_threshold
        ) if self.config.merge.use_clustering else None
        
        # Background subtractor
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            history=400, varThreshold=40, detectShadows=True
        )
        
        # ĞĞ±ÑŠĞµĞºÑ‚Ñ‹
        self.objects: Dict[int, TrackedObject] = {}
        self.next_id = 0
        self.colors = self._generate_colors(50)
        
        # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ (Ğ´Ğ»Ñ merge)
        self.prev_detection_velocities: Dict[int, Tuple[float, float]] = {}
        self.prev_detections: List[Dict] = []
        
        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        self.stats = {
            'raw_detections': 0,
            'after_merge': 0,
            'confirmed_objects': 0
        }
    
    def _generate_colors(self, n: int) -> List[Tuple[int, int, int]]:
        colors = []
        for i in range(n):
            hue = (i * 0.618033988749895) % 1.0
            rgb = colorsys.hsv_to_rgb(hue, 0.85, 0.95)
            colors.append(tuple(int(c * 255) for c in rgb[::-1]))
        return colors
    
    def process_frame(self, frame: np.ndarray) -> List[TrackedObject]:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ´Ñ€"""
        h, w = frame.shape[:2]
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STAGE 1: Background Subtraction
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ ÑÑ€ĞºĞ¾ÑÑ‚Ğ¸
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        brightness_mask = np.ones_like(gray) * 255
        brightness_mask[gray < self.config.brightness_min] = 0
        brightness_mask[gray > self.config.brightness_max] = 0
        
        # Background subtraction
        fg_mask = self.bg_subtractor.apply(frame)
        fg_mask[fg_mask == 127] = 0
        
        fg_mask = cv2.bitwise_and(fg_mask, brightness_mask)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STAGE 2: ĞœĞ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ (ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if self.config.merge.use_morphological_merge:
            # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°
            kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel_small)
            
            # Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ close Ğ´Ğ»Ñ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ñ‚ĞµĞ»Ğ°
            kernel_large = cv2.getStructuringElement(
                cv2.MORPH_ELLIPSE, 
                (self.config.merge.morph_close_size, self.config.merge.morph_close_size)
            )
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel_large)
            
            # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel_small)
        else:
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel, iterations=2)
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STAGE 3: ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ñ‹
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, 
                                       cv2.CHAIN_APPROX_SIMPLE)
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ
        detections = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if area < self.config.min_area or area > self.config.max_area:
                continue
            
            x, y, cw, ch = cv2.boundingRect(contour)
            
            # Aspect ratio
            aspect = cw / ch if ch > 0 else 0
            if aspect < self.config.min_aspect_ratio or aspect > self.config.max_aspect_ratio:
                continue
            
            M = cv2.moments(contour)
            if M["m00"] > 0:
                cx = M["m10"] / M["m00"]
                cy = M["m01"] / M["m00"]
            else:
                cx, cy = x + cw / 2, y + ch / 2
            
            obj_mask = np.zeros((h, w), dtype=np.uint8)
            cv2.drawContours(obj_mask, [contour], -1, 255, -1)
            
            detections.append({
                'centroid': (cx, cy),
                'bbox': (x, y, cw, ch),
                'mask': obj_mask,
                'area': area
            })
        
        self.stats['raw_detections'] = len(detections)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STAGE 4: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ (Ğ´Ğ»Ñ merge)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        detection_velocities = self._estimate_detection_velocities(detections)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STAGE 5: MERGE Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if self.config.merge.merge_enabled and len(detections) > 1:
            detections = self.merger.merge_detections(detections, detection_velocities)
        
        # ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾: ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        if self.clusterer and len(detections) > 1:
            detections = self.clusterer.cluster(detections, detection_velocities)
        
        self.stats['after_merge'] = len(detections)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°
        self.prev_detections = detections
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STAGE 6: Ğ¢Ñ€ĞµĞºĞ¸Ğ½Ğ³
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        for obj in self.objects.values():
            obj.frames_lost += 1
        
        self._match_detections(detections)
        
        # ĞŸĞ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹
        for obj in self.objects.values():
            if not obj.confirmed and obj.frames_seen >= self.config.confirm_frames:
                obj.confirmed = True
                self.stats['confirmed_objects'] += 1
        
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ½Ğ½Ñ‹Ğµ
        self._cleanup_lost()
        
        return [obj for obj in self.objects.values() if obj.confirmed]
    
    def _estimate_detection_velocities(self, detections: List[Dict]) -> Dict[int, Tuple[float, float]]:
        """ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ ĞºĞ°Ğ´Ñ€Ğ¾Ğ¼"""
        velocities = {}
        
        if not self.prev_detections:
            return velocities
        
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞ¼Ñƒ ÑĞ¾ÑĞµĞ´Ñƒ
        for i, det in enumerate(detections):
            min_dist = float('inf')
            best_vel = (0, 0)
            
            for prev_det in self.prev_detections:
                dist = np.sqrt(
                    (det['centroid'][0] - prev_det['centroid'][0])**2 +
                    (det['centroid'][1] - prev_det['centroid'][1])**2
                )
                if dist < min_dist and dist < 150:
                    min_dist = dist
                    best_vel = (
                        det['centroid'][0] - prev_det['centroid'][0],
                        det['centroid'][1] - prev_det['centroid'][1]
                    )
            
            if min_dist < 150:
                velocities[i] = best_vel
        
        return velocities
    
    def _match_detections(self, detections: List[Dict]):
        """Ğ¡Ğ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸"""
        if not detections:
            return
        
        used = set()
        
        for obj in list(self.objects.values()):
            if obj.centroid is None:
                continue
            
            predicted = obj.predict_position() or obj.centroid
            
            min_dist = float('inf')
            best_idx = -1
            
            for i, det in enumerate(detections):
                if i in used:
                    continue
                
                dist = min(
                    np.sqrt((obj.centroid[0] - det['centroid'][0])**2 +
                           (obj.centroid[1] - det['centroid'][1])**2),
                    np.sqrt((predicted[0] - det['centroid'][0])**2 +
                           (predicted[1] - det['centroid'][1])**2)
                )
                
                # Velocity check
                if dist > self.config.max_velocity * 1.5:
                    continue
                
                if dist < min_dist and dist < 120:
                    min_dist = dist
                    best_idx = i
            
            if best_idx >= 0:
                det = detections[best_idx]
                obj.update(
                    det['centroid'], 
                    det['bbox'], 
                    det['mask'],
                    det.get('merged_from', 1)
                )
                used.add(best_idx)
        
        # ĞĞ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹
        for i, det in enumerate(detections):
            if i not in used:
                self._create_object(det)
    
    def _create_object(self, detection: Dict):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚"""
        obj = TrackedObject(
            id=self.next_id,
            color=self.colors[self.next_id % len(self.colors)]
        )
        obj.update(
            detection['centroid'], 
            detection['bbox'], 
            detection['mask'],
            detection.get('merged_from', 1)
        )
        
        if self.config.use_kalman:
            obj.init_kalman()
        
        self.objects[self.next_id] = obj
        self.next_id += 1
    
    def _cleanup_lost(self):
        """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹"""
        to_remove = [
            oid for oid, obj in self.objects.items()
            if obj.frames_lost > self.config.lost_frames_max or
               (not obj.confirmed and obj.frames_lost > 3)
        ]
        for oid in to_remove:
            del self.objects[oid]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¨ Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Visualizer:
    """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"""
    
    def __init__(self, show_debug: bool = False):
        self.show_debug = show_debug
        self.font = cv2.FONT_HERSHEY_SIMPLEX
    
    def render(self, frame: np.ndarray, objects: List[TrackedObject],
               tracker: AdvancedTrackerV2, frame_idx: int,
               total_frames: int, fps: float) -> np.ndarray:
        
        output = frame.copy()
        
        # ĞœĞ°ÑĞºĞ¸
        for obj in objects:
            if obj.mask is not None:
                overlay = output.copy()
                overlay[obj.mask > 0] = obj.color
                cv2.addWeighted(overlay, 0.35, output, 0.65, 0, output)
                
                contours, _ = cv2.findContours(obj.mask, cv2.RETR_EXTERNAL,
                                              cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(output, contours, -1, obj.color, 2)
        
        # Ğ¢Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        for obj in objects:
            if len(obj.trajectory) > 1:
                pts = list(obj.trajectory)
                for i in range(1, len(pts)):
                    alpha = i / len(pts)
                    color = tuple(int(c * alpha) for c in obj.color)
                    thickness = max(1, int(3 * alpha))
                    cv2.line(output,
                            tuple(map(int, pts[i-1])),
                            tuple(map(int, pts[i])),
                            color, thickness)
        
        # Bboxes
        for obj in objects:
            if obj.bbox:
                x, y, w, h = obj.bbox
                
                corner = min(20, w // 5, h // 5)
                corners = [
                    [(x, y), (x + corner, y)],
                    [(x, y), (x, y + corner)],
                    [(x + w, y), (x + w - corner, y)],
                    [(x + w, y), (x + w, y + corner)],
                    [(x, y + h), (x + corner, y + h)],
                    [(x, y + h), (x, y + h - corner)],
                    [(x + w, y + h), (x + w - corner, y + h)],
                    [(x + w, y + h), (x + w, y + h - corner)],
                ]
                for p1, p2 in corners:
                    cv2.line(output, p1, p2, obj.color, 2)
                
                # ĞœĞµÑ‚ĞºĞ°
                label = f"#{obj.id}"
                if obj.merged_count > 1:
                    label += f" [M:{obj.merged_count}]"
                
                speed = obj.get_speed()
                if speed > 1:
                    label += f" v:{speed:.0f}"
                
                cv2.putText(output, label, (x, y - 8),
                           self.font, 0.5, (0, 0, 0), 3)
                cv2.putText(output, label, (x, y - 8),
                           self.font, 0.5, obj.color, 1)
        
        # Ğ¦ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´Ñ‹
        for obj in objects:
            if obj.smoothed_pos:
                center = tuple(map(int, obj.smoothed_pos))
                cv2.circle(output, center, 6, (255, 255, 255), 2)
                cv2.circle(output, center, 4, obj.color, -1)
        
        # Ğ˜Ğ½Ñ„Ğ¾-Ğ¿Ğ°Ğ½ĞµĞ»ÑŒ
        self._draw_info(output, objects, tracker, frame_idx, total_frames, fps)
        
        return output
    
    def _draw_info(self, frame: np.ndarray, objects: List[TrackedObject],
                   tracker: AdvancedTrackerV2, frame_idx: int,
                   total_frames: int, fps: float):
        
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (320, 140), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        y = 35
        cv2.putText(frame, "TRACKER v2 + MERGE", (20, y),
                   self.font, 0.7, (100, 200, 255), 2)
        
        y += 25
        cv2.putText(frame, f"Frame: {frame_idx + 1}/{total_frames} | FPS: {fps:.1f}", 
                   (20, y), self.font, 0.45, (200, 200, 200), 1)
        
        y += 22
        cv2.putText(frame, f"Raw detections: {tracker.stats['raw_detections']}", 
                   (20, y), self.font, 0.4, (200, 200, 200), 1)
        
        y += 18
        cv2.putText(frame, f"After merge: {tracker.stats['after_merge']}", 
                   (20, y), self.font, 0.4, (100, 255, 100), 1)
        
        y += 18
        cv2.putText(frame, f"Confirmed objects: {len(objects)}", 
                   (20, y), self.font, 0.4, (255, 200, 100), 1)
        
        y += 22
        progress = (frame_idx + 1) / total_frames
        bar_w = 280
        cv2.rectangle(frame, (20, y), (20 + bar_w, y + 8), (50, 50, 60), -1)
        cv2.rectangle(frame, (20, y), (20 + int(bar_w * progress), y + 8),
                     (100, 200, 255), -1)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¬ ĞŸĞ ĞĞ¦Ğ•Ğ¡Ğ¡ĞĞ 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ProcessorV2:
    """ĞŸÑ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾"""
    
    def __init__(self, config: Optional[TrackerConfig] = None, show_debug: bool = False):
        self.tracker = AdvancedTrackerV2(config)
        self.visualizer = Visualizer(show_debug)
    
    def process_video(self, input_path: str, output_path: str,
                      show_preview: bool = True,
                      max_frames: Optional[int] = None):
        
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open: {input_path}")
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if max_frames:
            total = min(total, max_frames)
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        merge_cfg = self.tracker.config.merge
        
        print(f"\n{'â•'*65}")
        print(f"  ğŸ­ ADVANCED TRACKER v2 + MERGE")
        print(f"{'â•'*65}")
        print(f"  ğŸ“ Input:  {input_path}")
        print(f"  ğŸ“ Output: {output_path}")
        print(f"  ğŸ“ Size:   {width}x{height} @ {fps:.1f} FPS")
        print(f"{'â”€'*65}")
        print(f"  ğŸ”— MERGE CONFIG:")
        print(f"     â€¢ Distance merge: {merge_cfg.merge_distance} px")
        print(f"     â€¢ Velocity merge: {'ON' if merge_cfg.velocity_merge_enabled else 'OFF'} (threshold: {merge_cfg.velocity_similarity_threshold})")
        print(f"     â€¢ IoU merge: {'ON' if merge_cfg.iou_merge_enabled else 'OFF'} (threshold: {merge_cfg.iou_threshold})")
        print(f"     â€¢ Vertical merge: {'ON' if merge_cfg.vertical_merge_enabled else 'OFF'}")
        print(f"     â€¢ Morph close: {merge_cfg.morph_close_size}px")
        print(f"{'â•'*65}")
        print(f"\n  Press Q to quit, P to pause\n")
        
        frame_times = []
        frame_idx = 0
        
        while frame_idx < total:
            ret, frame = cap.read()
            if not ret:
                break
            
            start = time.time()
            
            objects = self.tracker.process_frame(frame)
            
            current_fps = 1.0 / np.mean(frame_times[-30:]) if frame_times else fps
            output_frame = self.visualizer.render(
                frame, objects, self.tracker, frame_idx, total, current_fps
            )
            
            frame_time = time.time() - start
            frame_times.append(frame_time)
            
            out.write(output_frame)
            
            if show_preview:
                preview = cv2.resize(output_frame, (0, 0), fx=0.6, fy=0.6)
                cv2.imshow('Tracker v2 + Merge', preview)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\n  Interrupted")
                    break
                elif key == ord('p'):
                    cv2.waitKey(0)
            
            if frame_idx % 30 == 0:
                progress = (frame_idx + 1) / total * 100
                avg_fps = 1.0 / np.mean(frame_times[-30:]) if frame_times else 0
                raw = self.tracker.stats['raw_detections']
                merged = self.tracker.stats['after_merge']
                print(f"\r  [{progress:5.1f}%] FPS: {avg_fps:.1f} | Raw: {raw} â†’ Merged: {merged} | Objects: {len(objects)}", end="")
            
            frame_idx += 1
        
        print(f"\n\n{'â•'*65}")
        print(f"  âœ… COMPLETE")
        print(f"{'â•'*65}")
        if frame_times:
            print(f"  â±  Avg time: {np.mean(frame_times)*1000:.1f} ms")
            print(f"  ğŸš€ Avg FPS: {1.0/np.mean(frame_times):.1f}")
        print(f"  ğŸ’¾ Saved: {output_path}")
        print(f"{'â•'*65}\n")
        
        cap.release()
        out.release()
        cv2.destroyAllWindows()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ğŸ­ Advanced Tracker v2 + MERGE',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python advanced_tracker_v2.py test.mp4
  python advanced_tracker_v2.py test.mp4 --merge-distance 100
  python advanced_tracker_v2.py test.mp4 --velocity-threshold 0.9
  python advanced_tracker_v2.py test.mp4 --morph-close 20
        """
    )
    
    parser.add_argument('input', help='Input video')
    parser.add_argument('-o', '--output', default=None)
    
    # Merge parameters
    parser.add_argument('--merge-distance', type=float, default=80.0,
                       help='Max distance for merge (px)')
    parser.add_argument('--velocity-threshold', type=float, default=0.7,
                       help='Velocity similarity threshold (0-1)')
    parser.add_argument('--iou-threshold', type=float, default=0.1,
                       help='IoU threshold for merge')
    parser.add_argument('--morph-close', type=int, default=15,
                       help='Morphological close kernel size')
    
    # Disable features
    parser.add_argument('--no-velocity-merge', action='store_true')
    parser.add_argument('--no-iou-merge', action='store_true')
    parser.add_argument('--no-vertical-merge', action='store_true')
    parser.add_argument('--no-clustering', action='store_true')
    
    # General
    parser.add_argument('--min-area', type=int, default=1500)
    parser.add_argument('--confirm-frames', type=int, default=3)
    parser.add_argument('--no-preview', action='store_true')
    parser.add_argument('--max-frames', type=int, default=None)
    
    args = parser.parse_args()
    
    # Config
    merge_config = MergeConfig(
        merge_distance=args.merge_distance,
        velocity_similarity_threshold=args.velocity_threshold,
        iou_threshold=args.iou_threshold,
        morph_close_size=args.morph_close,
        velocity_merge_enabled=not args.no_velocity_merge,
        iou_merge_enabled=not args.no_iou_merge,
        vertical_merge_enabled=not args.no_vertical_merge,
        use_clustering=not args.no_clustering
    )
    
    config = TrackerConfig(
        min_area=args.min_area,
        confirm_frames=args.confirm_frames,
        merge=merge_config
    )
    
    if args.output is None:
        p = Path(args.input)
        args.output = str(p.parent / f"{p.stem}_merged.mp4")
    
    processor = ProcessorV2(config)
    processor.process_video(
        args.input,
        args.output,
        show_preview=not args.no_preview,
        max_frames=args.max_frames
    )


if __name__ == '__main__':
    main()

